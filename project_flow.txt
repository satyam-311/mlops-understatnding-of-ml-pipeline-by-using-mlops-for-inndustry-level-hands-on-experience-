Building Pipeline:
1> Create a GitHub repo and clone it in local (Add experiments).
2> Add src folder along with all components(run them individually).
3> Add data, models, reports directories to .gitignore file
4> Now git add, commit, push

Setting up dcv pipeline (without params)
5> Create dvc.yaml file and add stages to it.
6> dvc init then do "dvc repro" to test the pipeline automation. (check dvc dag)
7> Now git add, commit, push

Setting up dcv pipeline (with params)
8> add params.yaml file
9> Add the params setup (mentioned below)
10> Do "dvc repro" again to test the pipeline along with the params
11> Now git add, commit, push

Expermients with DVC:
12> pip install dvclive
13> Add the dvclive code block (mentioned below)
14> Do "dvc exp run", it will create a new dvc.yaml(if already not there) and dvclive directory (each run will be considered as an experiment by DVC)
15> Do "dvc exp show" on terminal to see the experiments or use extension on VSCode (install dvc extension)
16> Do "dvc exp remove {exp-name}" to remove exp (optional) | "dvc exp apply {exp-name}" to reproduce prev exp
17> Change params, re-run code (produce new experiments)
18> Now git add, commit, push



adding remote s3 storage to dvc

1. login to awsd console
2. create IAM user (straight forword process)
3.create s3 (enter unique name and create)
  pip install dvc[s3]
  pip install awscli
 # Initialize DVC in your repo – dvc init (not if already Initialize)
4.Configure AWS credentials – aws configure
5. Add S3 remote storage – dvc remote add -d myremote s3://mybucket/path
6. commit to remote – dvc commit
7. Push data to remote storage – dvc push

 >Pull data from remote – dvc pull
 >Verify remote list – dvc remote list